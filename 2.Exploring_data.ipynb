{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Twitter data\n",
    "In this notebook, we are going to explore our cleaned dataset that we created in the [first notebook.](1.Data_wrangling_and_cleaning.ipynb)\n",
    "\n",
    "We are going to do some exploratory analysis in order to understand the shape of the data, patterns and values, correlations between features, and hidden meaning behind our data.\n",
    "\n",
    "## Goal: \n",
    "Learn some common aspects of data exploration, calculate some statistics and visualize data column by column.\n",
    "\n",
    "## Introduction to exploratory data analysis\n",
    "[Exploratory data analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. (Wikipedia)\n",
    "\n",
    "\n",
    "### Data exploration process:\n",
    " - Computing summary statistics\n",
    " - Plotting\n",
    " - Additional wrangling if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load python modules\n",
    "\n",
    "There are many packages available in Python that provide a variety of functions for data science. Here we load the libraries needed for this notebook. \n",
    "\n",
    "Load the python libraries first. Additional libraries that we are going to use in this notebook are: \n",
    " - [wordcloud](https://amueller.github.io/word_cloud/) -  used to create wordclouds in Python\n",
    " - [nltk](https://www.nltk.org/) - natural language toolkit, library to work with language.\n",
    " - [folium](https://python-visualization.github.io/folium/) - library for creating maps.\n",
    " \n",
    "We'll also be using [pandas](https://pandas.pydata.org/) to work with DataFrames, one of the most common libraries used for data science. There are good tutorials available on pandas and worthwhile spending more time with in order to become more comfortable with data science in python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "import pandas as pd   #will reference pandas as pd\n",
    "\n",
    "import nltk\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt #will reference matplotlib as plt\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "except ImportError:\n",
    "    !pip install  --user wordcloud\n",
    "    from importlib import reload\n",
    "    import site\n",
    "    reload(site)\n",
    "    from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "try:\n",
    "    import folium\n",
    "    from folium.plugins import MarkerCluster\n",
    "except ImportError:\n",
    "    !pip install  --user folium\n",
    "    from importlib import reload\n",
    "    import site\n",
    "    reload(site)\n",
    "    import folium\n",
    "    from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we will read the cleaned dataset from helper git repo and display the first 5 rows\n",
    "\n",
    "There is a copy of the cleaned dataset (created in the first notebook) downloaded into the helper git repository `data-science-for-entrepreneurs-helper`.  \n",
    "\n",
    "We will read the csv file into pandas DataFrame and print the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name=\"./data/alberta_tweets_cleaned_feb_25.csv\"\n",
    "\n",
    "# reading 'created_at_date' column as timestamp\n",
    "tweets_cleaned = pd.read_csv(file_name, parse_dates=['created_at_date']) \n",
    "\n",
    "#timezone needs to be converted again\n",
    "tweets_cleaned['created_at_date']=tweets_cleaned['created_at_date'].dt.tz_localize('UTC').dt.tz_convert('MST')\n",
    "\n",
    "tweets_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Intro to pandas DataFrames\n",
    "<details>\n",
    "<summary> Click here to learn some basic operations on pandas DataFrames </summary>\n",
    "<br>\n",
    "\n",
    "`tweets_cleaned` is a DataFrame - 2dimentional data structure(similar to a table). We work with DataFrames in Python using pandas python library.    \n",
    "Note, there are a few different ways for accessing data inside a DataFrame. We'll cover a couple of aspects here, but you may also want to read more in the pandas documentation([here](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python) and \n",
    "[here](https://cmdlinetips.com/2018/02/how-to-subset-pandas-dataframe-based-on-values-of-a-column/)) or cheat sheet such as the one handed out [here](https://www.dataquest.io/blog/large_files/pandas-cheat-sheet.pdf). \n",
    "\n",
    "<h4> Basic operations with DataFrames</h4>\n",
    "\n",
    "<h5>Display on the screen</h5>\n",
    "\n",
    "**head()** - without arguments displays first 5 rows, or takes number of rows to display as an argument:    \n",
    "\n",
    "```python\n",
    "tweets_cleaned.head()   \n",
    "tweets_cleaned.head(10)\n",
    "```\n",
    "\n",
    "DataFrame name  will display the entire dataframe:   \n",
    "```python\n",
    "tweets_cleaned\n",
    "```\n",
    "\n",
    "Note, you might need to use **print()** function if there are multiple dataframes displayed in one code cell:  \n",
    "```python\n",
    "print(tweets_cleaned.head())\n",
    "```\n",
    "\n",
    "<h5>Select set of rows/colums</h5>\n",
    "\n",
    "Selecting one column:   \n",
    "```python\n",
    "tweets_cleaned['created_at_date']\n",
    "```\n",
    "\n",
    "Selecting multiple columns:  \n",
    "```python\n",
    "tweets_cleaned[['created_at_date','extended_tweet_cleaned']]  \n",
    "```\n",
    "\n",
    "Selecting one row(row 2):  \n",
    "```python\n",
    "tweets_cleaned.iloc[[2]]\n",
    "```  \n",
    "\n",
    "Selecting multiple rows(rows 2 and 5):   \n",
    "```python\n",
    "tweets_cleaned.iloc[[2,5]]\n",
    "``` \n",
    "\n",
    "Selecting range of rows(rows 2,3,4 and 5):   \n",
    "```python\n",
    "tweets_cleaned.iloc[[2:5]]\n",
    "```\n",
    "\n",
    "Selecting rows and columns:   \n",
    "```python\n",
    "tweets_cleaned[['created_at_date','extended_tweet_cleaned']].iloc[[2,5]]\n",
    "```  \n",
    "\n",
    "Note, row numbers start at 0.\n",
    "   \n",
    "<h5>Subset by value in specific column</h5>\n",
    " \n",
    "First we need to create a boolean expression for a subset(let's say we want to filter out rows where value of `user_location` is equal to `Calgary,AB` ):   \n",
    "\n",
    "```python\n",
    "bool_expression= tweets_cleaned['user_location']==\"Calgary,AB\"\n",
    "```\n",
    "\n",
    "\n",
    "**Boolean expression**  in this case is an expression that can be either **True** or **False** for every line in DataFrame, for example:   \n",
    "```python\n",
    "0        False  \n",
    "1        False  \n",
    "2        False  \n",
    "3        False  \n",
    "4        True  \n",
    "5        True  \n",
    "6        True  \n",
    "7        False  \n",
    "8        False  \n",
    "9        False  \n",
    "10       True  \n",
    "...\n",
    "```\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "Subset DataFrame by boolean expression:   \n",
    "```python\n",
    "tweets_cleaned[bool_expression].head(10)\n",
    "```\n",
    "\n",
    "**Other  examples of boolean expressions**:\n",
    "\n",
    "Not equal:   \n",
    "```python\n",
    "bool_expression = tweets_cleaned['user_location']!=\"Calgary,AB\"\n",
    "```\n",
    "\n",
    "Is part of list of values:\n",
    "```python\n",
    "bool_expression = tweets_cleaned['user_location'].isin([\"Calgary,AB\",\"Canada\"])\n",
    "```\n",
    "\n",
    "Multiple conditions  - \"and\":\n",
    "```python\n",
    "bool_expression = (tweets_cleaned['user_location']==\"Calgary,AB\") & (tweets_cleaned['name']!=\"Dave\")\n",
    "```\n",
    "\n",
    "Multiple conditions - \"or\":  \n",
    "```python\n",
    "bool_expression = (tweets_cleaned['user_location']==\"Calgary,AB\") | (tweets_cleaned['name']!=\"Dave\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "#### We will explore the data column by column \n",
    "A couple of things to keep in mind while doing this is what are we expecting from the data? Are our observations consistent with these expectations? If not, why do they not line up? Are there any trends, outliers, or interesting observations to make note of? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `extended_tweet_cleaned` column\n",
    "\n",
    "Let's begin with this column as this is likely the most interesting one to us. It contains all the text data tweeted out in each tweet. We would like to analyze this further. One way to examine the tweets is by looking at word frequencies. This could be done by using a bar chart or alternatively, using a word cloud. \n",
    "\n",
    "As you might imagine, if  we just go ahead and create a word cloud, it will be dominated by some very commonly used words, such as \"the\", \"or\", \"and\", etc. In order to prevent these common words from dominating the plot, we will remove them. Commonly used words that don't add anything contextually interesting such as this are called \"stopwords\". The wordcloud package that we will be using has a pre-built list of stopwords. \n",
    "\n",
    "Let's import them and examine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import  STOPWORDS\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the [WordCloud()](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud.WordCloud) function from the wordcloud library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "#In the following function, random_state is set for reproducibility and collocation=False means \n",
    "#that if the same word is encountered in collocation with different words,\n",
    "#it is still considered as one word (to remove duplicates)\n",
    "def wordcloud(tweets,col):\n",
    "    words=\" \".join([i for i in tweets[col]]) # we join all tweets in one character string \n",
    "    wordcloud = WordCloud(width=800, height=400,collocations=False,background_color=\"white\",stopwords=stopwords,random_state = 2019).generate(words)\n",
    "    plt.figure( figsize=(20,10))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(tweets_cleaned,'extended_tweet_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "All these words make a lot of sense, except for... amp?? Let's find some sample tweets that will allows us to inspect them why people might be tweeting about amps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 700)\n",
    "rows_containing_amp=tweets_cleaned['extended_tweet_cleaned'].str.contains(\"amp\")\n",
    "tweets_cleaned[rows_containing_amp]['extended_tweet_cleaned'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that &amp is just a shortcut for ampersand. This is a bit misleading and so let's delete all the &amp occurences from the 'extended_tweet_cleaned' column. \n",
    "\n",
    "What would be another way of dealing with the confusing &amp text? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned['extended_tweet_cleaned']=tweets_cleaned['extended_tweet_cleaned'].str.replace('&amp',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclude \"one\" and \"will\" words\n",
    "\"one\" and \"will\" words can possibly be excluded as well, they don't have any special meaning. In this case, we'll add them to the list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.add(\"one\")\n",
    "stopwords.add(\"will\")\n",
    "wordcloud(tweets_cleaned,'extended_tweet_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise1: add Calgary and AB (or other words as you like) to stopwords to exclude them from the wordcloud and try plotting the wordcloud again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### type something here\n",
    "\n",
    "\n",
    "###\n",
    "wordcloud(tweets_cleaned,'extended_tweet_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - this word cloud looks more sensible. What picture emerges from this? Are there any  themes we can identify? People seem to be thankful, something about time - maybe mentioning a good time? While we can guess at some of these aspects, it's impossible to say anything about the context these words occur in. For that, we would need some more sophisticated analyses. This could include aspects like n-gram analysis or topic modelling, which is what we will take a look at in the next notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Created_at_date` column\n",
    "This column is of type Timestamp. \n",
    "The following [features](https://www.geeksforgeeks.org/python-working-with-date-and-time-using-pandas/) can be useful for working with the Timestamp format:\n",
    " - **dt.year** returns the year of the date time.\n",
    " - **dt.month** returns the month of the date time.\n",
    " - **dt.day** returns the day of the date time.\n",
    " - **dt.hour** returns the hour of the date time.\n",
    " - **dt.minute** returns the minute of the date time.\n",
    "   \n",
    " **min(), max()** functions can be used with timestamp as well.   \n",
    "   \n",
    " Let's find out the time range first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Time range: \",min(tweets_cleaned[\"created_at_date\"]),\"-\",max(tweets_cleaned[\"created_at_date\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data from February 25th to March 4th.\n",
    "#### Let's group by day and calculate the number of tweets for each day.\n",
    "\n",
    "We will use the [groupby](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) function, which is a pandas function that groups rows into groups based on one or multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##size of the plot is 5/5 square to make it circle, not oval \n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#group by day\n",
    "tweets_cleaned_by_day=tweets_cleaned.groupby(tweets_cleaned[\"created_at_date\"].dt.day)\n",
    "\n",
    "#Select the 'created_at_date' column from grouped dataframe count up, and plot\n",
    "tweets_cleaned_by_day[\"created_at_date\"].count().plot(kind=\"pie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pie chart shows the proportion of tweets per day. The number on the circle corresponds to the day of the month the data was collected. \n",
    "\n",
    "Another way to plot this is as a bar chart: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)) ## making bigger plot rectangular shape in this case\n",
    "\n",
    "ax = tweets_cleaned_by_day[\"created_at_date\"].count().plot(kind=\"bar\")\n",
    "\n",
    "#Note in the line above that all we changed from the pie chart is the type of graph we requested\n",
    "ax.set_xlabel(\"Day\", size =16)\n",
    "ax.set_ylabel(\"Counts\", size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the most tweets took place on March 2nd. Let's confirm that and check out some additional summary stats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape[0] - gives number of rows, shape[1] - number of columns\n",
    "\n",
    "print(\"Total number of tweets: \",\n",
    "      tweets_cleaned.shape[0])\n",
    "\n",
    "#subsetting only rows where day is equal to 2\n",
    "bool_expression_day_2 = tweets_cleaned[\"created_at_date\"].dt.day==2\n",
    "\n",
    "print(\"Number of tweets collected on March 2nd: \", \n",
    "      tweets_cleaned[bool_expression_day_2].shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a look at the number of tweets by hour of day and whether this make sense with what we expect. What kind of pattern would you expect? \n",
    " \n",
    "#### Exercise2: Using the same methodology as for the bar chart above, can you plot the number of tweets by hour? \n",
    "Note this is a summary plot for the same hour for all days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type something here\n",
    "\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by hour\n",
    "tweets_cleaned_by_hour=tweets_cleaned.groupby(tweets_cleaned[\"created_at_date\"].dt.hour)\n",
    "\n",
    "print(\"Total number of tweets collected between 03:00 and 04:00: \", \n",
    "      tweets_cleaned_by_hour[\"created_at_date\"].count()[3])\n",
    "\n",
    "print(\"Total number of tweets collected between 18:00 and 19:00: \", \n",
    "      tweets_cleaned_by_hour[\"created_at_date\"].count()[18])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise3: plot tweets number by day and hour \n",
    "**Hint1**:   \n",
    "For two values in groupby() function:\n",
    ">instead of `groupby([value])` use `groupby([value1,value2])` \n",
    "\n",
    "Note how plot can be different if you do `groupby([day,hour])`  vs `groupby([hour,day])`.\n",
    "\n",
    "**Hint2**: \n",
    ">Use `figsize=(20,10)` to make larger plot\n",
    "\n",
    "**Optional**:  \n",
    "Use this code snippet to display every 4th x axis label, to make plot more clear:\n",
    ">``` python\n",
    "  for n, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    ">     if n % 4 != 0:\n",
    ">        label.set_visible(False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## type something here\n",
    "\n",
    "\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - so far everything looks as we expected it for when the tweets were collected and at which time of day users were  most active. Let's move on to the next column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Hashtag_string`  and `user_string` columns\n",
    "These two columns are of type String and have a list of hastags/user mentions separated by a blank.  \n",
    "We will create a list of all the hashtags first using the [join()](https://www.tutorialspoint.com/python/string_join.htm) and [split()](https://www.w3schools.com/python/ref_string_split.asp) functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashtags=\" \".join([i for i in tweets_cleaned['hashtags_string']]) #join all the hashtags into one character sting\n",
    "all_hashtags=all_hashtags.split()  # transform into a list\n",
    "print(all_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the [FreqDist()](https://kite.com/python/docs/nltk.probability.FreqDist) function from the nltk library to get frequency distributions for all the words.  That is, this function will count how many times each hashtag occurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freque_dist=nltk.FreqDist(all_hashtags)\n",
    "print(\"Most common hashtags: \",freque_dist.most_common(20)) ## most_common(n) function prints top n words with highest frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize this using matplotlib to print the top 25 most common hashtags. It's possible to use the [plot()](https://kite.com/python/docs/nltk.probability.FreqDist.plot) function directly with a  FreqDist object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)) # plot size\n",
    "plt.title(\"Most common hashtags\", size = 20)\n",
    "plt.xlabel(\"Hashtag\", size = 16)\n",
    "plt.ylabel(\"Counts\", size = 16)\n",
    "plt.xticks(fontsize=14) ## Change font for x axis labels\n",
    "freque_dist.plot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same steps for the user_string column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_users=\" \".join([i for i in tweets_cleaned['user_string']])\n",
    "all_users=all_users.split()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Most common user mentions\", size=20)\n",
    "plt.xlabel(\"Tagged User\", size=16)\n",
    "plt.ylabel(\"Number of tags\", size =16)\n",
    "plt.xticks(fontsize=14)\n",
    "fd = nltk.FreqDist(all_users)\n",
    "fd.plot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many users talked about the most commonly mentioned user? \n",
    "We will use the [str.contains()](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.Series.str.contains.html) function to get rows containing specific string:\n",
    "> tweets_cleaned['user_string'].str.contains(\"JustinTrudeau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_expression_JT=tweets_cleaned['user_string'].str.contains(\"JustinTrudeau\")\n",
    "\n",
    "print(\"Number of tweets with the most common user mention 'JustinTrudeau':\",\n",
    "      len(tweets_cleaned[bool_expression_JT]['name']))\n",
    "\n",
    "\n",
    "print(\"Number of users using the most common user mention 'JustinTrudeau':\",\n",
    "      len(tweets_cleaned[bool_expression_JT]['name'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contrast, how does this compare to the second most commonly mentioned user? Check the bar chart above and find out how many users mentioned it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bool_expression_BTS=tweets_cleaned['user_string'].str.contains(\"BTS_twt\")\n",
    "\n",
    "print(\"Number of tweets with the most common user mention 'BTS_twt':\",\n",
    "      len(tweets_cleaned[bool_expression_BTS]['name']))\n",
    "\n",
    "print(\"Number of users using the most common user mention 'BTS_twt':\",\n",
    "      len(tweets_cleaned[bool_expression_BTS]['name'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which users used the most number of user mentions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_user_mentions=tweets_cleaned['user_string'].str.split().str.len().max()\n",
    "\n",
    "rows_with_max_user_mentions=tweets_cleaned['user_string'].str.split().str.len()==max_user_mentions\n",
    "\n",
    "print(\"Maximum number of user mentions:\",\n",
    "      max_user_mentions,\n",
    "      \"made by  \",\n",
    "      tweets_cleaned[rows_with_max_user_mentions] ['name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise4 - try subsetting the data by hour (for example between 9 and 10 am) and plot the most common hashtags or user_mentions\n",
    "Hint: to select only data between 9 and 10 am use the dt.hour==9 boolean expression:\n",
    ">bool_expression_hour_9=tweets_cleaned[\"created_at_date\"].dt.hour==9   \n",
    ">tweets_cleaned[bool_expression_hour_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type something here\n",
    "\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### `User_location` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_locations=tweets_cleaned[\"user_location\"].unique()\n",
    "\n",
    "print(\"Number of unique user locations:\", len(unique_locations), \"\\n\")\n",
    "print(unique_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1000 unique user_locations. These are locations that are entered by users in the profile and open to pretty much any input, which is why we see some odd locations in there. Not sure if this is overly interesting right now. Let's look into latitude/longitude instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Latitude/longitude` columns\n",
    "We will subset data by day (to make it faster) and plot tweets that have coordinates on a map.\n",
    "\n",
    "How are latitude/longitude added to a tweet? Do all tweets have them? How reliable is this data source? A few things to keep in mind as we go through this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of tweets: \",tweets_cleaned.shape[0])\n",
    "\n",
    "rows_with_location=tweets_cleaned[\"longitude\"].notnull()\n",
    "tweets_have_location=tweets_cleaned[rows_with_location]\n",
    "\n",
    "print(\"Number of tweets having location data: \",tweets_have_location.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_have_location_day_2=tweets_have_location[\"created_at_date\"].dt.day==2\n",
    "\n",
    "tweets_subset_march2=tweets_have_location.loc[rows_have_location_day_2]\n",
    "print(\"Number of tweets having location data for March 2: \",tweets_subset_march2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the location data on a map and use the library [folium](https://github.com/python-visualization/folium). We will iterate through each row in the December 18 subset of data and add the coordinates to the map where they exist. \n",
    "\n",
    "This can be accomplished using the [iterrrows()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html) function to iterate through dataframe rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "calgary_coords = [51.0486, -114.0708] # Calgary coordinates, (where we are going to center the map)\n",
    "my_map = folium.Map(location = calgary_coords, zoom_start = 13)\n",
    "\n",
    "for index,row in tweets_subset_march2.iterrows(): \n",
    "        folium.Marker([row[\"longitude\"], row[\"latitude\"]]).add_to(my_map) \n",
    "my_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like these locations are in line with what we expected. The highest distribution falls within the downtown core and if you scroll out, you'll notice that the scraper seems to have done a good job limiting the collected tweets to roughly Alberta.\n",
    "\n",
    "#### Exercise5: try subsetting by user mention (e.g \"calgarylibrary\") and  plot tweets on a map\n",
    "Hint: use str.contains() function applied to tweets_have_location['user_string'] colum:\n",
    ">bool_expression_usermentions=tweets_have_location['user_string'].str.contains(\"your_search_word\")\n",
    ">tweets_have_location[bool_expression_usermentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### type something here\n",
    "\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Screen_name` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some summary stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Total number of tweets: \",tweets_cleaned.shape[0])\n",
    "print(\"Total number of users: \",tweets_cleaned['screen_name'].unique().shape[0])\n",
    "print(\"Top 20 most active users:\")\n",
    "tweets_grouped_by_screenname=tweets_cleaned[\"screen_name\"].groupby(tweets_cleaned[\"screen_name\"])\n",
    "tweets_grouped_by_screenname.count().reset_index(name='count').sort_values(['count'], ascending=False).head(20)#.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to csv\n",
    "We have done a whole lot of exploring and some additonal data wrangling that has changed the data, so let's save an updated dataset to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned.to_csv('alberta_tweets_cleaned_feb_25_updated.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusion\n",
    "\n",
    "We've done some data exploration in order to try to understand the data better. Our time spent on each column depended on how complex the data is each column. Since we will continue to work with the text data, that is where we started and spent most of our time working with. \n",
    "\n",
    "Take-aways: \n",
    "* Data exploration helps provide a sense of the data that is there and often reveals interesting trends and patterns \n",
    "* Always consider both exploring the data using summary statistics along with data visualizations\n",
    "* Think about what you would expect the data to look like going into it and determine whether your assumptions hold up. If not, why not? This could tell you important things about your data. \n",
    "* Subject matter expertise is invaluable in data science, including in the exploration phase. A solid understanding of how Twitter is used, or working with someone who does, can often quickly resolve questions that could otherwise take hours to answer. \n",
    "\n",
    "Next, we will go deeper into natural language processing, will  build topic models  and do sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.add(\"Calgary\")\n",
    "stopwords.add(\"Alberta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by hour\n",
    "tweets_cleaned_by_hour=tweets_cleaned.groupby(tweets_cleaned[\"created_at_date\"].dt.hour)\n",
    "\n",
    "#Select the 'created_at_date' column from grouped dataframe count up, and plot\n",
    "ax = tweets_cleaned_by_hour[\"created_at_date\"].count().plot(kind=\"bar\", figsize=(10,5))\n",
    "ax.set_xlabel(\"Hour\", size =16)\n",
    "ax.set_ylabel(\"Counts\", size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = tweets_cleaned[\"created_at_date\"].groupby([tweets_cleaned[\"created_at_date\"].dt.day,tweets_cleaned[\"created_at_date\"].dt.hour]).count().plot(kind=\"bar\", figsize=(20,10))\n",
    "ax.set_xlabel(\"Day/Hour\", size =16)\n",
    "ax.set_ylabel(\"Counts\", size=16)\n",
    "##To display every 4th tick for x axis:\n",
    "for n, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "    if n % 4 != 0:\n",
    "        label.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_expression_hour_9=tweets_cleaned[\"created_at_date\"].dt.hour==9\n",
    "tweets_cleaned_9=tweets_cleaned[bool_expression_hour_9]\n",
    "\n",
    "all_users=\" \".join([i for i in tweets_cleaned_9['user_string']])\n",
    "all_users=all_users.split()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Most common user mentions between 9 and 10 am\", size=20)\n",
    "plt.xlabel(\"Tagged User\", size=16)\n",
    "plt.ylabel(\"Number of tags\", size =16)\n",
    "plt.xticks(fontsize=14)\n",
    "fd = nltk.FreqDist(all_users)\n",
    "fd.plot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_expression_usermentions=tweets_have_location['user_string'].str.contains(\"calgarylibrary\") \n",
    "tweets_subset_library=tweets_have_location[bool_expression_usermentions]\n",
    "\n",
    "my_map = folium.Map(location = calgary_coords, zoom_start = 13)\n",
    "\n",
    "for index,row in tweets_subset_library.iterrows():  \n",
    "        folium.Marker([row[\"longitude\"], row[\"latitude\"]]).add_to(my_map) \n",
    "my_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
